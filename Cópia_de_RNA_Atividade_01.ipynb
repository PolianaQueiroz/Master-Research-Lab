{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PolianaQueiroz/Master-Research-Lab/blob/main/C%C3%B3pia_de_RNA_Atividade_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crie um modelo perceptron e aplique para prever os seguintes casos:\n",
        "1. Caso do AND\n",
        "2. Caso do OR\n",
        "3. Caso do XOR\n",
        "\n",
        "Para cada caso defina sua própria estratégia de inicialização dos pesos. Comente os resultados obtidos em cada caso."
      ],
      "metadata": {
        "id": "qFY1LZF799kW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_BeIG7N95Qy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26daa8c-13d4-45e6-be76-5d6bb7c13d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TESTANDO O CASO: AND ---\n",
            "Treinando... (Pesos Iniciais: [0. 0.], Bias Inicial: 0.0)\n",
            "Treino concluído. (Pesos Finais: [0.2 0.1], Bias Final: -0.20000000000000004)\n",
            "\n",
            "Resultados:\n",
            "Entrada | Desejado | Previsto | Correto?\n",
            "------------------------------------------\n",
            "[0 0]  |    0     |     0    |   Sim\n",
            "[0 1]  |    0     |     0    |   Sim\n",
            "[1 0]  |    0     |     0    |   Sim\n",
            "[1 1]  |    1     |     1    |   Sim\n",
            "\n",
            "Acurácia: 100.0%\n",
            "Comentário: Sucesso! O Perceptron convergiu e aprendeu a função.\n",
            "\n",
            "--- TESTANDO O CASO: OR ---\n",
            "Treinando... (Pesos Iniciais: [0. 0.], Bias Inicial: 0.0)\n",
            "Treino concluído. (Pesos Finais: [0.1 0.1], Bias Final: -0.1)\n",
            "\n",
            "Resultados:\n",
            "Entrada | Desejado | Previsto | Correto?\n",
            "------------------------------------------\n",
            "[0 0]  |    0     |     0    |   Sim\n",
            "[0 1]  |    1     |     1    |   Sim\n",
            "[1 0]  |    1     |     1    |   Sim\n",
            "[1 1]  |    1     |     1    |   Sim\n",
            "\n",
            "Acurácia: 100.0%\n",
            "Comentário: Sucesso! O Perceptron convergiu e aprendeu a função.\n",
            "\n",
            "--- TESTANDO O CASO: XOR ---\n",
            "Treinando... (Pesos Iniciais: [0. 0.], Bias Inicial: 0.0)\n",
            "Treino concluído. (Pesos Finais: [-0.1  0. ], Bias Final: 0.0)\n",
            "\n",
            "Resultados:\n",
            "Entrada | Desejado | Previsto | Correto?\n",
            "------------------------------------------\n",
            "[0 0]  |    0     |     1    |   NÃO\n",
            "[0 1]  |    1     |     1    |   Sim\n",
            "[1 0]  |    1     |     0    |   NÃO\n",
            "[1 1]  |    0     |     0    |   Sim\n",
            "\n",
            "Acurácia: 50.0%\n",
            "Comentário: Falha! O Perceptron não conseguiu aprender esta função.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# PASSO 1: CRIAR O MODELO PERCEPTRON\n",
        "# ----------------------------------------------------------------\n",
        "# Vamos criar uma classe que define o Perceptron.\n",
        "# Ele terá um \"learning rate\" (taxa de aprendizado) e \"epochs\" (número de épocas de treino).\n",
        "# A inicialização dos pesos será feita dentro do método 'fit' (treinar).\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, n_epochs=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.weights = None  # Pesos (w1, w2)\n",
        "        self.bias = None     # Viés (b)\n",
        "\n",
        "    def _step_function(self, z):\n",
        "        # Esta é a função de ativação degrau (Heaviside)\n",
        "        # Retorna 1 se z >= 0, e 0 se z < 0\n",
        "        return np.where(z >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Esta é a função de treino.\n",
        "        Aqui, definimos nossa estratégia de inicialização dos pesos.\n",
        "\n",
        "        Estratégia de Inicialização:\n",
        "        Para simplificar e garantir reprodutibilidade, vamos iniciar os pesos e o bias com ZERO.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Inicialização dos pesos e bias\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0.0\n",
        "\n",
        "        print(f\"Treinando... (Pesos Iniciais: {self.weights}, Bias Inicial: {self.bias})\")\n",
        "\n",
        "        # Loop de treinamento (regra de aprendizado do Perceptron)\n",
        "        for _ in range(self.n_epochs):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                # 1. Calcular a saída líquida (z)\n",
        "                z = np.dot(x_i, self.weights) + self.bias\n",
        "\n",
        "                # 2. Aplicar a função de ativação para prever (y_pred)\n",
        "                y_pred = self._step_function(z)\n",
        "\n",
        "                # 3. Calcular o erro e atualizar os pesos\n",
        "                # A mágica do aprendizado está aqui:\n",
        "                # Se y_pred == y_true (acertou), o erro (y[idx] - y_pred) é 0, e nada muda.\n",
        "                # Se errou, o erro é 1 ou -1, e os pesos/bias são ajustados.\n",
        "                update = self.learning_rate * (y[idx] - y_pred)\n",
        "\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "        print(f\"Treino concluído. (Pesos Finais: {self.weights}, Bias Final: {self.bias})\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Esta função usa os pesos treinados para prever novos dados.\"\"\"\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return self._step_function(z)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# PASSO 2: DEFINIR OS DADOS DE TREINO\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# Entradas (X) - Os 4 casos possíveis\n",
        "# (0,0), (0,1), (1,0), (1,1)\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Saídas desejadas (y) para cada caso\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "y_or = np.array([0, 1, 1, 1])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "\n",
        "def run_test(case_name, X, y_true):\n",
        "    \"\"\"Função auxiliar para treinar e testar cada caso.\"\"\"\n",
        "    print(f\"\\n--- TESTANDO O CASO: {case_name} ---\")\n",
        "\n",
        "    # 1. Criar uma nova instância do Perceptron\n",
        "    # (Usamos a mesma inicialização de pesos/bias = 0 para todos os casos)\n",
        "    perceptron = Perceptron(learning_rate=0.1, n_epochs=100)\n",
        "\n",
        "    # 2. Treinar o modelo com os dados específicos (X, y_true)\n",
        "    perceptron.fit(X, y_true)\n",
        "\n",
        "    # 3. Fazer as previsões com o modelo treinado\n",
        "    y_pred = perceptron.predict(X)\n",
        "\n",
        "    # 4. Mostrar os resultados\n",
        "    print(\"\\nResultados:\")\n",
        "    print(\"Entrada | Desejado | Previsto | Correto?\")\n",
        "    print(\"------------------------------------------\")\n",
        "\n",
        "    total_correct = 0\n",
        "    for i in range(len(X)):\n",
        "        is_correct = \"Sim\" if y_true[i] == y_pred[i] else \"NÃO\"\n",
        "        if is_correct == \"Sim\":\n",
        "            total_correct += 1\n",
        "        print(f\"{X[i]}  |    {y_true[i]}     |     {y_pred[i]}    |   {is_correct}\")\n",
        "\n",
        "    print(f\"\\nAcurácia: {total_correct / len(X) * 100}%\")\n",
        "\n",
        "    if total_correct == len(X):\n",
        "        print(\"Comentário: Sucesso! O Perceptron convergiu e aprendeu a função.\")\n",
        "    else:\n",
        "        print(\"Comentário: Falha! O Perceptron não conseguiu aprender esta função.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# PASSO 3: RODAR OS TESTES\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# Caso 1: AND\n",
        "run_test(\"AND\", X, y_and)\n",
        "\n",
        "# Caso 2: OR\n",
        "run_test(\"OR\", X, y_or)\n",
        "\n",
        "# Caso 3: XOR\n",
        "run_test(\"XOR\", X, y_xor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resolvendo o caso XOR"
      ],
      "metadata": {
        "id": "EQ8wim1qSBW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Vamos importar o MLPClassifier do Scikit-learn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# Vamos usar o 'warnings' para suprimir avisos de convergência que podem aparecer\n",
        "# em problemas simples, mas que não significam que o código falhou.\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# PASSO 1: DEFINIR OS DADOS (igual ao anterior)\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# Entradas (X) - Os 4 casos possíveis\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Saída desejada (y) para o XOR\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "print(f\"--- RESOLVENDO O CASO XOR COM MLP (Scikit-learn) ---\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# PASSO 2: CRIAR E TREINAR O MODELO MLP\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# Estratégia de Inicialização (Definição da Arquitetura):\n",
        "# Vamos criar um MLP com uma camada oculta.\n",
        "\n",
        "# MLPClassifier(...)\n",
        "#   hidden_layer_sizes=(4,):\n",
        "#       Esta é a parte mais importante!\n",
        "#       Define a arquitetura. (4,) significa UMA camada oculta com 4 neurônios.\n",
        "#       Dois neurônios são o mínimo teórico para resolver o XOR, mas 4 é mais robusto.\n",
        "#\n",
        "#   activation='relu':\n",
        "#       A função de ativação não-linear (Rectified Linear Unit).\n",
        "#       É o que permite ao MLP aprender relações complexas.\n",
        "#\n",
        "#   solver='adam':\n",
        "#       Um otimizador de aprendizado eficiente.\n",
        "#\n",
        "#   max_iter=5000:\n",
        "#       Número máximo de épocas de treino. O XOR é complexo e precisa de mais treino.\n",
        "#\n",
        "#   random_state=1:\n",
        "#       Define uma \"semente\" para a inicialização aleatória dos pesos.\n",
        "#       Isso garante que nós dois teremos o MESMO resultado ao rodar o código.\n",
        "#       O treino de redes neurais é sensível à inicialização dos pesos.\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(4,),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=5000,\n",
        "    random_state=1,\n",
        "    learning_rate_init=0.01 # Uma taxa de aprendizado um pouco maior ajuda\n",
        ")\n",
        "\n",
        "# Suprimir o aviso de convergência\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
        "\n",
        "    # Treinar o modelo\n",
        "    print(\"Treinando o MLP...\")\n",
        "    mlp.fit(X, y_xor)\n",
        "    print(\"Treino concluído.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# PASSO 3: FAZER AS PREVISÕES E MOSTRAR OS RESULTADOS\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# Fazer as previsões com o modelo treinado\n",
        "y_pred = mlp.predict(X)\n",
        "\n",
        "# Mostrar os resultados\n",
        "print(\"\\nResultados do XOR com MLP:\")\n",
        "print(\"Entrada | Desejado | Previsto | Correto?\")\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "total_correct = 0\n",
        "for i in range(len(X)):\n",
        "    is_correct = \"Sim\" if y_xor[i] == y_pred[i] else \"NÃO\"\n",
        "    if is_correct == \"Sim\":\n",
        "        total_correct += 1\n",
        "    print(f\"{X[i]}  |    {y_xor[i]}     |     {y_pred[i]}    |   {is_correct}\")\n",
        "\n",
        "print(f\"\\nAcurácia: {total_correct / len(X) * 100}%\")\n",
        "\n",
        "if total_correct == len(X):\n",
        "    print(\"Comentário: Sucesso! O MLP (com sua camada oculta) conseguiu aprender a função não-linear XOR.\")\n",
        "else:\n",
        "    print(\"Comentário: Falha. O modelo não convergiu. Tente aumentar 'max_iter' ou alterar 'random_state'.\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# (Opcional) Ver os pesos que o modelo aprendeu\n",
        "# ----------------------------------------------------------------\n",
        "print(\"\\n--- Pesos aprendidos pela rede ---\")\n",
        "print(\"Pesos da camada oculta (Input -> Oculta):\")\n",
        "print(mlp.coefs_[0])\n",
        "print(\"\\nBias da camada oculta:\")\n",
        "print(mlp.intercepts_[0])\n",
        "print(\"\\nPesos da camada de saída (Oculta -> Saída):\")\n",
        "print(mlp.coefs_[1])\n",
        "print(\"\\nBias da camada de saída:\")\n",
        "print(mlp.intercepts_[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNmK_jbvSPGs",
        "outputId": "fe192507-70bd-43d4-d6c8-8502935bcefe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RESOLVENDO O CASO XOR COM MLP (Scikit-learn) ---\n",
            "Treinando o MLP...\n",
            "Treino concluído.\n",
            "\n",
            "Resultados do XOR com MLP:\n",
            "Entrada | Desejado | Previsto | Correto?\n",
            "------------------------------------------\n",
            "[0 0]  |    0     |     0    |   Sim\n",
            "[0 1]  |    1     |     1    |   Sim\n",
            "[1 0]  |    1     |     1    |   Sim\n",
            "[1 1]  |    0     |     0    |   Sim\n",
            "\n",
            "Acurácia: 100.0%\n",
            "Comentário: Sucesso! O MLP (com sua camada oculta) conseguiu aprender a função não-linear XOR.\n",
            "\n",
            "--- Pesos aprendidos pela rede ---\n",
            "Pesos da camada oculta (Input -> Oculta):\n",
            "[[-2.98400588e-11  2.22919159e+00  1.36246054e-07 -2.46290385e+00]\n",
            " [ 7.74654930e-10 -2.22937221e+00 -5.88229378e-10  2.46227216e+00]]\n",
            "\n",
            "Bias da camada oculta:\n",
            "[-0.20646505  0.00038942 -0.16161097 -0.00095499]\n",
            "\n",
            "Pesos da camada de saída (Oculta -> Saída):\n",
            "[[-2.48482558e-10]\n",
            " [ 3.65749299e+00]\n",
            " [ 4.03442818e-07]\n",
            " [ 3.36408809e+00]]\n",
            "\n",
            "Bias da camada de saída:\n",
            "[-3.98301229]\n"
          ]
        }
      ]
    }
  ]
}